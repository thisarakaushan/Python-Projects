{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b0a416",
   "metadata": {},
   "source": [
    "# Web Content Aggregator(web scrapper)\n",
    "\n",
    "Create a web content aggregator that allows users to input URLs of their favorite news websites, blogs, or information sources. The aggregator will scrape the content from these URLs and present it in a user-friendly interface, allowing users to quickly access and read the latest content from various sources in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2dbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape data from the website\n",
    "def scrape_data():\n",
    "    url = url_input.get()\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('title').get_text()\n",
    "    paragraphs = soup.find_all('p')\n",
    "    data = {\n",
    "        'title': title,\n",
    "        'paragraphs': []\n",
    "    }\n",
    "    for paragraph in paragraphs:\n",
    "        data['paragraphs'].append(paragraph.get_text())\n",
    "    # Display the scraped data in the GUI\n",
    "    title_label.config(text=data['title'])\n",
    "    paragraphs_text.delete('1.0', tk.END)\n",
    "    for paragraph in data['paragraphs']:\n",
    "        paragraphs_text.insert(tk.END, paragraph + '\\n')\n",
    "\n",
    "# Create the GUI\n",
    "root = tk.Tk()\n",
    "root.title('Web Scraper')\n",
    "\n",
    "root.geometry(\"1200x600\")\n",
    "\n",
    "# URL input field\n",
    "url_label = tk.Label(root, text='Website URL:')\n",
    "url_label.pack()\n",
    "url_input = tk.Entry(root, width=50)\n",
    "url_input.pack()\n",
    "\n",
    "# Button to start scraping\n",
    "scrape_button = tk.Button(root, text='Scrape', command=scrape_data)\n",
    "scrape_button.pack()\n",
    "\n",
    "# Title label\n",
    "title_label = tk.Label(root, text='Title')\n",
    "title_label.pack()\n",
    "\n",
    "# Text area to display the scraped paragraphs\n",
    "paragraphs_label = tk.Label(root, text='Paragraphs')\n",
    "paragraphs_label.pack()\n",
    "paragraphs_text = tk.Text(root,width=120, height=25)\n",
    "paragraphs_text.pack()\n",
    "\n",
    "# Start the GUI\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a436ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed92f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e248c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fc51c8a",
   "metadata": {},
   "source": [
    "## Improved by adding error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa00fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Function to scrape data from the website\n",
    "def scrape_data():\n",
    "    url = url_input.get()\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        message = f\"Error: {e}\"\n",
    "        error_label.config(text=message)\n",
    "        return\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('title').get_text()\n",
    "    paragraphs = soup.find_all('p', {'class': 'content'})\n",
    "    data = {\n",
    "        'title': title,\n",
    "        'paragraphs': []\n",
    "    }\n",
    "    for paragraph in paragraphs:\n",
    "        data['paragraphs'].append(paragraph.get_text())\n",
    "    # Display the scraped data in the GUI\n",
    "    title_label.config(text=data['title'])\n",
    "    paragraphs_text.delete('1.0', tk.END)\n",
    "    for paragraph in data['paragraphs']:\n",
    "        paragraphs_text.insert(tk.END, paragraph + '\\n')\n",
    "    # Store the data in JSON and CSV formats\n",
    "    filename = 'scraped_data'\n",
    "    with open(f\"{filename}.json\", 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    with open(f\"{filename}.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Title', 'Paragraph'])\n",
    "        for paragraph in data['paragraphs']:\n",
    "            writer.writerow([data['title'], paragraph])\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title('Web Scraper')\n",
    "\n",
    "root.geometry(\"900x500\")\n",
    "\n",
    "url_label = tk.Label(root, text='Website URL:')\n",
    "url_label.pack()\n",
    "url_input = tk.Entry(root, width=50)\n",
    "url_input.pack()\n",
    "\n",
    "scrape_button = tk.Button(root, text='Scrape', command=scrape_data)\n",
    "scrape_button.pack()\n",
    "\n",
    "# Error message label\n",
    "error_label = tk.Label(root, fg='red')\n",
    "error_label.pack()\n",
    "\n",
    "title_label = tk.Label(root, text='Title')\n",
    "title_label.pack()\n",
    "\n",
    "# Text area to display the scraped paragraphs\n",
    "paragraphs_label = tk.Label(root, text='Paragraphs')\n",
    "paragraphs_label.pack()\n",
    "paragraphs_text = tk.Text(root, width=100, height=20)\n",
    "paragraphs_text.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
